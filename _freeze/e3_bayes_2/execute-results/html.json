{
  "hash": "8848feee2dbe93442a07281b0c19a1cf",
  "result": {
    "markdown": "---\ntitle: \"Week #3\"\nauthor: \"Adrian Correndo & Josefina Lacasa\"\nformat:\n  html:\n    fontsize: 0.8em\n    linestretch: 1\n---\n\n\n# Introduction to Bayesian Stats #2\n\nThis is a follow-up article from [Bayes#1](https://adriancorrendo.github.io/statasaurusweb/code_2_bayes_1.html). Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.\n\n::: callout-note\n**Today's Topics**:\n\nComputing posterior distributions:\n\n#1. Acceptance/Rejection Sampling Basics:\n\n#2. Markov Chain Monte Carlo (MCMC) -more efficient than AR sampling-.\n\nPackages for Bayesian analysis in R:\n\n#3. brms\n\n#4. rstan\n\n#5. rjags\n:::\n\n## Packages to use today\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)\n```\n:::\n\n\n## Computing posterior distributions:\n\n### 1. Acceptance/Rejection Sampling Basics:\n\n1\\. Generate proposal parameter values\n\n2\\. Generate data with those parameters\n\n3\\. Compare the simulated data with the observed data = \"difference\"\n\n4\\. \"**Accept**\" that combination of parameters if the difference \\< predefined acceptable error. \"**Reject**\" if the difference \\> predefined acceptable error.\n\nSee an example:\n\nUsing data of yield vs plant density in corn:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n$$ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2$$\n\n\n\n\n\n\n\n\n1.  Generate proposal parameter values **using the prior ditributions**:\n\n\n$$\\beta_0 \\sim uniform(4, 6)$$\n\n$$\\beta_1 \\sim uniform(1, 3)$$\n\n$$\\beta_2 \\sim uniform(0.5, 2)$$\n\n$$\\sigma \\sim Gamma(2, 2)$$\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(567)\nb0_try <- runif(1, 4, 6)  # Parameter model\nb1_try <- runif(1, 1, 3)  # Parameter model \nb2_try <- rgamma(1, .5, 2) # Mathematical equation for process model\nmu_try <- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try <- rgamma(1, 2, 2)\n```\n:::\n\n\n2.  Generate data with those parameters\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(567)\ny_try <- rnorm(n, mu_try, sigma_try)  # Process model\n```\n:::\n\n\n3.  Compare the simulated data with the observed data = \"difference\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] <- sum(abs(y - y_try))\n```\n:::\n\n\n\n\n4.  \"**Accept**\" (gold) that combination of parameters if the difference \\< predifined acceptable error. \"**Reject**\" (red) if the difference \\> predifined acceptable error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Observed yield\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] < error, \"gold\", \"tomato\"))\n```\n\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/demo 1e-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/demo 1f-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/demo 1g-1.png){width=672}\n:::\n:::\n\n\nNow, what if whe change the priors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNow, do many tries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (k in 1:K_tries) {\n    \n    b0_try <- runif(1, 2, 10)  # Parameter model\n    b1_try <- rnorm(1, 2.2, .5)  # Parameter model \n    b2_try <- rgamma(1, .25, 2) # Mathematical equation for process model\n    mu_try <- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try <- rgamma(1, 2, 2)\n\n    y_try <- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] <- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] <- y_try\n    \n    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)\n}\n```\n:::\n\n\nAcceptance rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(which(diff < error))/K_tries\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.031291\n```\n:::\n:::\n\n\nPriors versus posteriors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/post 2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/post 3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(y_hat[which(diff < error), 25], col = \"grey\", freq = FALSE)\nabline(v = y[25], col = 'gold', lty = \"dashed\", lwd = 5)\n```\n\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLet's get started\n\n## 2. Markov Chain Monte Carlo\n\n![](images/Handbook.jpg){width=\"241\"}\n\nMarkov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics (Brooks et al., 2011).\n\nIn a nutshell, MCMC represents a family of algorithms that facilitate the generation of random samples from probability distributions that are difficult (e.g. high-dimensional) to sample directly. They are \"chains\" because the random samples are produced in consecutive-dependent steps (i.e. step 2 comes from step 1, step 3 comes from step 2, ....). This details is a game-changer to more efficiently use and integrate Monte Carlos simulations.\n\nSources on MCMC:\n\n<https://www.mcmchandbook.net/>\n\n<https://cran.r-project.org/package=MCMCpack>\n\n<https://cran.r-project.org/package=mcmc>\n\n<https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243>\n\n::: {align=\"center\"}\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Qqz5AJjyugM\" frameborder=\"0\" allowfullscreen>\n\n</iframe>\n:::\n\n## 3. *brms*: Bayesian Regression Models using \"Stan\"\n\n![](images/brms.png)\n\nDocumentation: <https://paul-buerkner.github.io/brms/>\n\nBug-reports: <https://github.com/paul-buerkner/brms/issues>\n\n*brms* is a very handy R-package that facilitates running Bayesian models using a relatively simple syntax. It is basically and interface that runs \"Stan\" behind the scenes. It uses a syntax quite similar to the [lme4](https://cran.r-project.org/package=lme4) package.\n\nIt allows to use several different type of distributions and link functions for models that are linear, counts, survival, response, ordinal, zero-inflated, etc.\n\nDue to its relatively simple syntax, today, we are going to start our Bayesian coding with brms.\n\nMore about brms at <https://www.jstatsoft.org/article/view/v080i01>\n\n![](images/paste-6C740B97.png){width=\"336\"}\n\n### Fit brms\n\nLet's fit the example using the brms package.\n\n### brms pars\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n```\n:::\n\n\n### Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| eval: false\n#| echo: true\n\n# 01. Run models\n\nbayes_model <- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n```{.r .cell-code}\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\nbayes_model <- readRDS(file = \"assets/class03/bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n```\n\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Visualize model results\nbayes_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 46) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     6.16      0.90     4.36     7.96 1.00     2462     2456\nB1_Intercept     1.93      0.26     1.42     2.45 1.00     2386     2619\nB2_Intercept     0.11      0.02     0.08     0.15 1.00     2395     2591\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.75      0.08     0.60     0.93 1.00     2969     2807\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code}\n# Compare vs traditional linear model (lm)\ndata_frame_q <- data_frame %>% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n     6.1324       1.9401      -0.1127  \n```\n:::\n:::\n\n\n### Using posterior distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictions\nm1 <- data_frame %>% \n  ungroup() %>% \n  dplyr::select(x) %>% \n  group_by(x) %>% filter(x == max(x)) %>% \n  ungroup() %>% unique() %>% rename(max = x) %>% \n  # Generate a sequence of x values\n  mutate(data = max %>% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %>% \n  unnest() %>% dplyr::select(-max) %>%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %>% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %>% ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n```\n:::\n\n```{.r .cell-code}\n# Summarize\nm1_quantiles <- m1 %>% \n  group_by(x) %>% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n# Plot\nm1_plot <- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk3\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"#8a817c\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T)+\n  scale_color_viridis_d()+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_bw()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot\n```\n\n::: {.cell-output-display}\n![](e3_bayes_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## 4. *rstan*: R interface to \"Stan\"\n\n![](images/stanlogo.png){width=\"190\"}\n\nDocumentation: <https://mc-stan.org/rstan/>\n\nBug reports: <https://github.com/stan-dev/rstan/issues/>\n\n*stan* is a stand-alone open-source software platform designed for statistical modeling using high-performance statistical computation applying its own language. When selecting the Bayesian computational approach (i.e. ***rejection sampling criteria***) there are several alternatives to choose. *Stan* produces Bayesian statistical inference following Hamiltonian Monte Carlo (HMC), and No-U-Turn Samples (NUTS). Besides R, *stan* has interfaces with other popular languages such as Python, MATLAB, Julia.\n\nIn contrast to *brms*, stan's syntax is more complicated for begginers, but the positive side is that requires us to write the statistical model.\n\nWe will not fit a model directly with stan today, but *brms* brings a function that allows users to obtain the code to run the analysis by ourselves using rstan. Let's see...\n\n## 5. *rjags*: R interface to \"Just Another Gibbs Sampler\"\n\n![](images/1601161_JAGS.png){width=\"318\"}\n\nDocumentation: <https://mcmc-jags.sourceforge.io/>\n\nBug reports: <https://sourceforge.net/projects/mcmc-jags/>\n\n*rjags* is another popular option for Bayesian statistical inference following MCMC using R. *Rjags* produces Bayesian statistical inference following BUGS language (WinBUGS). Similar to *stan*, *rjags* it is probably not for beginner, since it requires us to write out the statistical model (although it is always ideal). To extract the posteriors, it also requires [coda](https://cran.r-project.org/web/packages/coda/index.html), which is especially designed for summarizing and plotting MCMC simulations.\n",
    "supporting": [
      "e3_bayes_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}