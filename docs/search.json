[
  {
    "objectID": "e1_intro.html",
    "href": "e1_intro.html",
    "title": "Week #1",
    "section": "",
    "text": "This contains notes about the introductory meeting, intended to:\n\nReview on the use of GitHub for collaborative projects.\nExplore the new features offered by the new development of RStudio: Quarto.\n\n\n\nGitHub is an internet hosting service designed for software development and version control using Git. Basically, GitHub facilitates to work in a collaborative manner through multiple features designed to efficiently control changes made by users during a working session.\nUseful desktop applications:\n\nGitHub Desktop\nGitKraken\n\nKey glossary:\n\n\n\n\n\n\n\n\n#\nConcept\nDescription\n\n\n\n\n1\nversion control\nis the practice of tracking and managing changes to software code\n\n\n2\nrepository\nis a code hosting platform for version control and collaboration.\n\n\n3\nbranch\nis a unique set of code changes with a unique name.\n\n\n5\nclone\nis to copy a remote repository into your local\n\n\n4\nremote/local\nthey are Git repositories hosting code at different locations. ‚ÄúRemote‚Äù means located on the internet or another network. ‚ÄúLocal‚Äù means located in your machine.\n\n\n5\norigin\nis the remote version of a given repository.\n\n\n6\nmain/master\nis the name of the default branch. In the past ‚Äúmaster‚Äù was the default. Since 2020, ‚Äúmain‚Äù is the default name of all new source code.\n\n\n7\ncommit\nis a ‚Äúrevision‚Äù or individual set of changes to a file or group of files within a branch.\n\n\n8\npull / fetch\nis the action to fetch and download content from a remote repository to update your local repository.\n\n\n9\npush\nis the action to upload content from a local repository to update its remote version (all within a given branch).\n\n\n10\npull request\nis to let the repository owner to know about changes you‚Äôve made to his code pushing to a new named branch. It allows to discuss, review, accept/reject the changes, and eventually accept and merge your branch to update the old version (normally, the ‚Äúmain‚Äù branch).\nNote: This action allows users to appear into the ‚Äúcontributors‚Äù list of a repository.\n\n\n11\nmerge\nis the action of combining two branches into one, and putting a combined ‚Äúforked history‚Äù.\n\n\n12\nrebase\nis moving or combining a sequence of commits to a new base commit.\n\n\n13\nfast-forward\nis merging when there is a direct linear path from the source branch (updating from) to the target branch (the one updating to).\n\n\n14\nstash*\nit temporarily shelves changes you‚Äôve made to your copy so you can work on something else, and then come back and re-apply them later. *very useful when you need to pull and update from the remote but you have been working on your local file.\n\n\n15\nrevert\nis the ‚Äúundo‚Äù action to safely undoing changes. It creates a new commit that inverses changes.\n\n\n16\nfork\nis to copy others‚Äô repository into your account. Your forked repo will act as an independent branch, so you can commit changes and put pull request from the forked repo.\n\n\n17\nIssues\nis a section of any GitHub repository that allows to track issues experienced by users. Particularly useful for packages. Issues can be quoted and linked to pull requests.\n\n\n18\nPATs\nstands for Personal Access Tokens, which are an alternative authentication method to passwords and SSH\n\n\n19\nlfs\nthe lfs stands for Large File Storage. It allows you to track heavy files that are not allowed by default.\n\n\n20\nGitHub Actions\nis the way to automate continuous integration and delivery (CI/CD). Particularly, CI is a very important a software practice for package development, since it allows to detect errors on different operative systems via multiple automated tests.\n\n\n\n\n\n\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It allows to combine R, Python, Julia, and Java (Observable JS) for the design of reports, applications, websites, blogs, scientific publications, and more...\nKey concepts to review:\n\nqmd\nrender\nyml\ncss\n\nLearn more: https://quarto.org\n\n\n\nDownload Quarto or the latest version of RStudio\n\n\n\n\nToday, we will work together on the creation of our own hex-sticker using both Quarto and GitHub.\n\n\n\nClone a repository:\n\nOpen your web-browser and go to https://github.com/adriancorrendo/Stats_Hub_2022\nFork to your own GitHub account.\nGo to your forked repo and click on the ‚ÄúCode‚Äù button and copy the https address\n\n\nOpen Rstudio and go to File > New Project‚Ä¶ > version control > Git, and paste the link.\n\nGo to the logo > code and open the hexsticker_stats_hub.qmd file.\nInstall the required packages.\nModify the logos or create an alternative using the *.qmd file.\nCreate a commit. with your changes.\nPush (send changes).\nContribute with a Pull Request.\nWait for the PR to be merged into the original repo."
  },
  {
    "objectID": "e2_bayes_1.html",
    "href": "e2_bayes_1.html",
    "title": "Week #2",
    "section": "",
    "text": "This article is intended to provide a brief introduction to key concepts about Bayesian theory and differences with the traditional Frequentist approach:\n\n\n\n\n\n\nImportant\n\n\n\nNeither of both, Frequentist or Bayesian, are always the most suitable solution for your analysis. However‚Ä¶.üòâ\n\n\nFor this reason, today we are going to discuss and compare both approaches.\nLet‚Äôs watch some short videos about it\n\n\n\n\n\n\nWhat do you think?\n\nOpen discussion‚Ä¶.\n\n\n\n\n\nPerhaps, the main disagreement between Frequentism and Bayesianism is about the TRUTH.\nThe Frequentism vision is heavily rooted on the actual existence of the TRUTH. Every time we estimate a model‚Äôs parameter, we expect to approximate to a true value. It is named ‚Äúfrequentism‚Äù because it is based on the frequency of repeated events.\nFor example, if we want to assess the probability of getting a #6 when rolling a dice, Frequentism says that ‚ÄúIf we roll a dice close to infinite times, the proportion of #6 out of the total number of rolls will approach 16.7% (the theoretical probability)‚Äù. Thus, Frequentism makes inference conditional to an ideal, ‚Äútheoretical‚Äù condition of repeating the same experiment infinite times. In other words, conclusions rely on events we have not observed.\nThe Bayesian approach instead, DOES NOT assume the existence of the TRUTH. In contrast, it is based on PROBABILITIES & BELIEFS.\nPROBABILITIES: For the Bayesian vision, everything is a matter of probability. Any fact or result about an ‚Äúestimate‚Äù could range from extremely unlikely to extremely likely. However, anything is considered completely true or false.\nBELIEFS: here is probably the most important point of the Bayesian vision. Bayesian models allow to introduce (and update) prior knowledge on a topic, introducing our own certainty or uncertainty about events. EVEN IF WE DON‚ÄôT KNOW ANYTHING about it (spoiler alter: uninformative prior!).\nBayesianism considers probability as an expression of the degree of certainty (or uncertainty).\nFollowing the same example with the dice roll, Bayesian interpretation says that ‚Äúwe are, a priori, 16.7% certain we are going to get a #6‚Äù. The, Bayesianism makes inference conditional to the data we observed. We basically test the likelihood of a prior hypothesis being true given the observed data, and we generate a new ‚Äúlikelihood‚Äù of updated hypothesis being true given the observed data.\nAnd now, to compare previous beliefs (prior) to updated knowledge (posterior) we can introduce the concept of Bayes Factor, which is a ratio between to candidate statistical models represented by marginal likelihood to measure the support for one model over the other. For example, if we have 0.167 as a prior belief of obtaining a #6, and our updated likelihood (after combining with observed data) results 0.334:\n\\[\nBayes Factor = \\frac{0.334}{0.167} = 2\n\\]\nThus, our updated hypothesis is twice as likely to be true than our prior hypothesis given the observed data.\nTherefore, when we analyze our data:\n\nFrequentism assumes models being fixed and our data random (maximum likelihood, conditional to theoretical events).\nBayesian assumes that models can vary around our data (conditional to observed data)\n\n\n\n\n\n\n\nTip\n\n\n\nFor simple models, however, the two approaches would be practically indistinguishable‚Ä¶\nHINT: think about uninformative prior knowledge!\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe structure of the Bayesian theory is similar to the Human Logic process:\n(i) we have some data,\n(ii) we have beliefs about the underlying process,\n(iii) combining both, we can update our beliefs.\n\n\n\n\n\n\\[ P(A_{true} | B)  = \\frac{P(B|A_{true}) * P(A_{true})}{P(B)}\\] \\[ Posterior  = \\frac{Likelihood * Prior}{Evidence}\\]\n\n\n\n\n\n\n\n\n\n\nPriors are basically a formalization of our believes in a form of a mathematical function describing a ‚Äúdistribution‚Äù. First, it depends on the nature of the variable of interest, which could be ‚Äúdiscrete‚Äù or ‚Äúcontinuous‚Äù. Second, it depends on what we know (or not) about the process of interest‚Ä¶\n\n\n\nThere is a very important difference between Frequentism and Bayesianism in terms of error interpretation. Let‚Äôs say we estimate 95% confidence and credible intervals for \\(\\theta\\):\n\nConfidence intervals (Frequentist): ‚ÄúIf we repeat the experiment infinite times, 95% of the estimated confidence intervals will contain the true value of \\(\\theta\\) (based on repeated measurements)‚Äú. Note that since \\(\\theta\\) is fixed, it can only be within or outside the interval.\nCredible intervals (Bayesian): In contrast, Bayesianism has a ‚Äúliteral‚Äù interpretation of the error saying, *‚Äúthere is a probability of 95% that the parameter* \\(\\theta\\) lies within this credible interval‚Äù. This is a range of probable values. Note:‚ÄùGiven that the prior is correct‚Äù‚Ä¶\n\n\n\n\n\n\n\nBayesian Models: A Statistical Primer for Ecologists. Hobbs and Hooten\nBringing Bayesian Models to Life. Hooten and Hefley\nNot from biological sciences but still very good:\nStatistical Rethinking. McElreath\nOn Bayesian workflow/philosophy:\nBayesian workflow\nScientific Reasoning: The Bayesian Approach. Howson and Urbach\n\n\n\nBayesian Data Analysis 3\n\n\n\nMakowski et al., 2020\n\n\n\nBlog: Statistical Modeling, Causal Inference, and Social Science. Gelman et al.\nPodcast: Learning Bayesian Statistics"
  },
  {
    "objectID": "e3_bayes_2.html",
    "href": "e3_bayes_2.html",
    "title": "Week #3",
    "section": "",
    "text": "This is a follow-up article from Bayes#1. Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.\n\n\n\n\n\n\nNote\n\n\n\nToday‚Äôs Topics:\nComputing posterior distributions:\n#1. Acceptance/Rejection Sampling Basics:\n#2. Markov Chain Monte Carlo (MCMC) -more efficient than AR sampling-.\nPackages for Bayesian analysis in R:\n#3. brms\n#4. rstan\n#5. rjags\n\n\n\n\n\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)\n\n\n\n\n\n\n1. Generate proposal parameter values\n2. Generate data with those parameters\n3. Compare the simulated data with the observed data = ‚Äúdifference‚Äù\n4. ‚ÄúAccept‚Äù that combination of parameters if the difference < predefined acceptable error. ‚ÄúReject‚Äù if the difference > predefined acceptable error.\nSee an example:\nUsing data of yield vs plant density in corn:\n\n\n\n\n\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2\\]\n\nGenerate proposal parameter values using the prior ditributions:\n\n\\[\\beta_0 \\sim uniform(4, 6)\\]\n\\[\\beta_1 \\sim uniform(1, 3)\\]\n\\[\\beta_2 \\sim uniform(0.5, 2)\\]\n\\[\\sigma \\sim Gamma(2, 2)\\]\n\nset.seed(567)\nb0_try <- runif(1, 4, 6)  # Parameter model\nb1_try <- runif(1, 1, 3)  # Parameter model \nb2_try <- rgamma(1, .5, 2) # Mathematical equation for process model\nmu_try <- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try <- rgamma(1, 2, 2)\n\n\nGenerate data with those parameters\n\n\n\nset.seed(567)\ny_try <- rnorm(n, mu_try, sigma_try)  # Process model\n\n\nCompare the simulated data with the observed data = ‚Äúdifference‚Äù\n\n\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] <- sum(abs(y - y_try))\n\n\n‚ÄúAccept‚Äù (gold) that combination of parameters if the difference < predifined acceptable error. ‚ÄúReject‚Äù (red) if the difference > predifined acceptable error.\n\n\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Observed yield\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] < error, \"gold\", \"tomato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, what if whe change the priors:\n\n\n\n\n\nNow, do many tries\n\nfor (k in 1:K_tries) {\n    \n    b0_try <- runif(1, 2, 10)  # Parameter model\n    b1_try <- rnorm(1, 2.2, .5)  # Parameter model \n    b2_try <- rgamma(1, .25, 2) # Mathematical equation for process model\n    mu_try <- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try <- rgamma(1, 2, 2)\n\n    y_try <- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] <- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] <- y_try\n    \n    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)\n}\n\nAcceptance rate\n\nlength(which(diff < error))/K_tries\n\n[1] 0.031291\n\n\nPriors versus posteriors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist(y_hat[which(diff < error), 25], col = \"grey\", freq = FALSE)\nabline(v = y[25], col = 'gold', lty = \"dashed\", lwd = 5)\n\n\n\n\n\n\n\n\n\nLet‚Äôs get started\n\n\n\n\n\nMarkov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics (Brooks et al., 2011).\nIn a nutshell, MCMC represents a family of algorithms that facilitate the generation of random samples from probability distributions that are difficult (e.g.¬†high-dimensional) to sample directly. They are ‚Äúchains‚Äù because the random samples are produced in consecutive-dependent steps (i.e.¬†step 2 comes from step 1, step 3 comes from step 2, ‚Ä¶.). This details is a game-changer to more efficiently use and integrate Monte Carlos simulations.\nSources on MCMC:\nhttps://www.mcmchandbook.net/\nhttps://cran.r-project.org/package=MCMCpack\nhttps://cran.r-project.org/package=mcmc\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243\n\n\n\n\n\n\n\n\nDocumentation: https://paul-buerkner.github.io/brms/\nBug-reports: https://github.com/paul-buerkner/brms/issues\nbrms is a very handy R-package that facilitates running Bayesian models using a relatively simple syntax. It is basically and interface that runs ‚ÄúStan‚Äù behind the scenes. It uses a syntax quite similar to the lme4 package.\nIt allows to use several different type of distributions and link functions for models that are linear, counts, survival, response, ordinal, zero-inflated, etc.\nDue to its relatively simple syntax, today, we are going to start our Bayesian coding with brms.\nMore about brms at https://www.jstatsoft.org/article/view/v080i01\n\n\n\nLet‚Äôs fit the example using the brms package.\n\n\n\n\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n\n\n#| eval: false\n#| echo: true\n\n# 01. Run models\n\nbayes_model <- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n\nCompiling Stan program...\n\n\nStart sampling\n\n# 02. Save object\n# saveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\n# Load from file\nbayes_model <- readRDS(file = \"assets/class03/bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n\n\n\n# Visualize model results\nbayes_model\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 46) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     6.16      0.90     4.36     7.96 1.00     2462     2456\nB1_Intercept     1.93      0.26     1.42     2.45 1.00     2386     2619\nB2_Intercept     0.11      0.02     0.08     0.15 1.00     2395     2591\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.75      0.08     0.60     0.93 1.00     2969     2807\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Compare vs traditional linear model (lm)\ndata_frame_q <- data_frame %>% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n     6.1324       1.9401      -0.1127  \n\n\n\n\n\n\n# Create predictions\nm1 <- data_frame %>% \n  ungroup() %>% \n  dplyr::select(x) %>% \n  group_by(x) %>% filter(x == max(x)) %>% \n  ungroup() %>% unique() %>% rename(max = x) %>% \n  # Generate a sequence of x values\n  mutate(data = max %>% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %>% \n  unnest() %>% dplyr::select(-max) %>%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %>% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %>% ungroup()\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n# Summarize\nm1_quantiles <- m1 %>% \n  group_by(x) %>% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n# Plot\nm1_plot <- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk3\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"#8a817c\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T)+\n  scale_color_viridis_d()+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_bw()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot\n\n\n\n\n\n\n\n\n\nDocumentation: https://mc-stan.org/rstan/\nBug reports: https://github.com/stan-dev/rstan/issues/\nstan is a stand-alone open-source software platform designed for statistical modeling using high-performance statistical computation applying its own language. When selecting the Bayesian computational approach (i.e.¬†rejection sampling criteria) there are several alternatives to choose. Stan produces Bayesian statistical inference following Hamiltonian Monte Carlo (HMC), and No-U-Turn Samples (NUTS). Besides R, stan has interfaces with other popular languages such as Python, MATLAB, Julia.\nIn contrast to brms, stan‚Äôs syntax is more complicated for begginers, but the positive side is that requires us to write the statistical model.\nWe will not fit a model directly with stan today, but brms brings a function that allows users to obtain the code to run the analysis by ourselves using rstan. Let‚Äôs see‚Ä¶\n\n\n\n\nDocumentation: https://mcmc-jags.sourceforge.io/\nBug reports: https://sourceforge.net/projects/mcmc-jags/\nrjags is another popular option for Bayesian statistical inference following MCMC using R. Rjags produces Bayesian statistical inference following BUGS language (WinBUGS). Similar to stan, rjags it is probably not for beginner, since it requires us to write out the statistical model (although it is always ideal). To extract the posteriors, it also requires coda, which is especially designed for summarizing and plotting MCMC simulations."
  },
  {
    "objectID": "e4_bayes_3.html",
    "href": "e4_bayes_3.html",
    "title": "Week #4",
    "section": "",
    "text": "This article is a review and final example to wrap up our brief introduction to Bayesian data analysis.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUnder the Bayesian approach we consider probability as an expression of the degree of certainty (or uncertainty) about a process or parameter.\n\n\n\n\n\n\n\n\nNote\n\n\n\nREMEMBER!\nThe structure of the Bayesian theory is similar to the Human Logic process. It is all about updating knowledge:\n(i) we have some data,\n(ii) we have beliefs about the underlying process,\n(iii) combining both, we can update our beliefs.\n\n\n\n\n\\[ P(\\theta | x)  = P(\\theta) * \\frac{P(x|\\theta)}{P(x)}\\] where \\(\\theta\\) is the parameter of interest, \\(x\\) is the data, and ‚Äú|‚Äù means ‚Äúconditional‚Äù.\n\\[ Knowledge~after  = Knowledge~before * updating~factor\\] \\[ Posterior~distribution  = Prior~distribution * \\frac{Likelihood} {Marginal Likelihood} \\]\n\n\n\nYou may have heard multiple times about Bayesian Hierarchical Framework or Bayesian Hierarchical Modelling. This simply means that our model is compound by multiple nested levels (hierarchy) where we account for random effects. Actually, they are usually also referred as multi-level models (Hooten & Hefley, 2019). There are two relevant concepts here to derive the posterior distributions:\n\nHyper-parameters are the parameters of the prior distribution. For example, if we have \\(Y|\\mu \\sim N(\\mu, \\sigma^2)\\), with \\(\\mu = \\beta_1 *X\\) as the ‚Äútop-level‚Äù parameter describing the process model, where \\(\\beta_1\\) is the hyper-parameter.\nHyper-priors are the distributions of the hyper-parameters. For example, \\(\\beta_1 \\sim N(\\mu_{\\beta_1}, \\sigma^2_{\\beta_1})\\) is the hyper-prior of \\(\\beta_1\\)\n\nBasically, we have a set of layers (see Kwag & Hu, 2019):\nLayer 1: the data layer \\[ y_i|\\mu_i, \\beta_1 \\sim P(y_i|\\mu_i, \\beta_1)\\]\nLayer 2: the process layer \\[ \\mu_i| \\beta_1 \\sim P(\\mu_i|\\beta_1)\\]\nLayer 3: the prior layer \\[ \\beta_1 \\sim P(\\beta_1) = \\]\n\\[ \\beta_1 \\sim N(\\mu_{\\beta_1}, \\sigma^2_{\\beta_1})\\]\n\n\n\nFollowing the example above, the posterior \\(P(\\mu_i, \\beta_1|y_i)\\) is a probability density function that quantifies the ‚Äúuncertainty‚Äù about \\(y_i\\) within a specific model after the data collection as follows:\n\\[ P(\\mu_i, \\beta_1|y_i)  \\propto P(y_i|\\mu_i, \\beta_1)*P(\\mu_i|\\beta_1) * P(\\beta_1) \\] where \\(y_i\\) represents the data, and \\(\\mu_i\\) is the process model dependent on the random variable \\(\\beta_1\\).\nIn practice, however, we never know this specific function. So what we do is to create multiple simulations (the MCMC) given the prior/s and the data, and then summarizing those simulations (e.g.¬†obtaining credible intervals at variable probability levels, for example, 95%).\n\n\n\nHooten, M.B., and Hefley, T.J., 2019. Chaper 19: Hierarchical Models. Bringing Bayesian Models to Life. https://doi.org/10.1201/9780429243653\nKwag, S., Ju, B.S. Application of a Bayesian hierarchical model to system identification of structural parameters. Engineering with Computers 36, 455‚Äì474 (2020). https://doi.org/10.1007/s00366-019-00708-1\n\n\n\n\n\n\n\nlibrary(easypackages)\npackages(\"dplyr\", \"tidyr\", \"purrr\", \"tibble\")\npackages(\"readxl\", \"stringr\")\npackages(\"ggplot2\")\npackages(\"nlme\", \"car\", \"emmeans\", \"multcomp\", \"multcompView\")\npackages(\"brms\", \"tidybayes\", \"performance\")\n\n# Data\n# Corn yield response to N rates somewhere in KS\ndata <- read_excel(\"data/cropyield.xlsx\") %>%\n  mutate(BLOCK = rep(c(1,2,3,4), nrow(.)/4))\n\n\n\n\n\n# Analisis, no intercept (0 + ...)\nyield_lme <- data %>% \n  group_by(PHASE) %>% nest() %>% \n  mutate(model = map(data,\n                     ~lme(GY_bu ~ 0 + as.factor(TREAT), \n                          random = ~1|BLOCK, data=.)))\n\n# Extract the model\ncorn_lme <- yield_lme[[\"model\"]][[1]]\n\ncorn_lme\n\nLinear mixed-effects model fit by REML\n  Data: . \n  Log-restricted-likelihood: -67.70787\n  Fixed: GY_bu ~ 0 + as.factor(TREAT) \n  as.factor(TREAT)0  as.factor(TREAT)60 as.factor(TREAT)120 as.factor(TREAT)180 \n           137.7226            183.1838            215.1319            233.2673 \nas.factor(TREAT)240 \n           236.2163 \n\nRandom effects:\n Formula: ~1 | BLOCK\n        (Intercept) Residual\nStdDev:     1.65409 17.46253\n\nNumber of Observations: 20\nNumber of Groups: 4 \n\n# ANOVA\nAnova(corn_lme, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: GY_bu\n                  Chisq Df Pr(>Chisq)    \nas.factor(TREAT) 2627.3  5  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# Multiple comparison\ncorn_lme_mc<-emmeans(corn_lme, ~ TREAT)\n\ncorn_lme_means <- as.data.frame(\n  cld(corn_lme_mc, decreasing = TRUE, details=FALSE, reversed=TRUE, alpha=0.05,  \n      adjust = \"tukey\", Letters=LETTERS))\n\ncorn_lme_means\n\n  TREAT   emmean       SE df lower.CL upper.CL .group\n1   240 236.2163 8.770349 12 209.5232 262.9093    A  \n2   180 233.2673 8.808076 12 206.4594 260.0751    A  \n3   120 215.1319 8.789152 12 188.3816 241.8822    AB \n4    60 183.1838 8.770349 12 156.4907 209.8768     B \n5     0 137.7226 8.789152 12 110.9724 164.4729      C\n\n\n\n\n\n\n\n\n\n# Set up pars\nWU = 1000\nIT = 3000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n\n\n# Analysis\ncorn_brms <- data %>%\n  mutate(TREAT = as.factor(TREAT)) %>%\n  dplyr::group_by(PHASE) %>% tidyr::nest() %>%\n  mutate(model = map(data,\n                     ~brms::brm(\n                       #Priors\n                       # Priors\n                       prior = c(\n    # Prior of the Yield (effect from 0) notice there is no intercept\n    prior(prior = 'normal(100,100)', class = \"b\") ),\n    # If using the intercept\n      # prior(prior = 'normal(80,80)', class = \"Intercept\") ),\n    # Careful with \"treatment\" prior, cause it's not yield now (it's response)\n      # prior(prior = 'normal(100,100)', class = \"Intercept\") ),\n    \n    # Formula\n    # With 0 as the intercept.\n    formula = bf(GY_bu ~  0 + TREAT + (1|BLOCK)),\n  data = ., sample_prior = \"yes\",\n  family = gaussian(link = 'identity'),\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1)  ) )\n\n\n\n\n\n# Plot\nplot(corn_brms$model[[1]]) \n\n\n\n\n\n\n# Check the posteriors \n# (yrep, blue lines) to see how the model simulates your data (y, black line)\npp_check(corn_brms$model[[1]], nsamples = 200)\n\n\n\n\n\n\n\n\n# Summary\nsummary(corn_brms$model[[1]])\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: GY_bu ~ 0 + TREAT + (1 | BLOCK) \n   Data: . (Number of observations: 20) \n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 5;\n         total post-warmup draws = 1600\n\nGroup-Level Effects: \n~BLOCK (Number of levels: 4) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)    11.68     12.34     0.36    41.57 1.00     1037      948\n\nPopulation-Level Effects: \n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nTREAT0     135.24     13.16   104.55   157.89 1.00     1122      986\nTREAT60    179.61     12.73   151.16   201.41 1.00     1269      963\nTREAT120   213.87     13.17   182.48   237.34 1.00     1288     1080\nTREAT180   226.12     14.07   194.25   250.38 1.00     1111      945\nTREAT240   232.10     13.33   200.74   256.62 1.00     1264     1196\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    18.43      3.99    12.50    27.90 1.00     1495     1534\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Performance\nperformance::performance(corn_brms$model[[1]])\n\n# Indices of model performance\n\nELPD    | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 | R2 (marg.) | R2 (adj.) | R2_adjusted_marginal |   RMSE\n-----------------------------------------------------------------------------------------------------------------\n-90.633 |   2.558 | 181.265 |    5.115 | 179.679 | 0.839 |      0.824 |     0.741 |               -0.365 | 13.943\n\n\n\n\n\n\n# Pass model draws to a dataframe\ncorn_brms_draws <- as.data.frame(corn_brms$model[[1]]) %>% \n  # Rename for simplicity\n  rename_with( ~ str_replace(.x, \"b_TREAT\", \"N\"))\n\n\n# Create a df with posterior of treatment differences\ncontrast_tibble <-\ncorn_brms_draws %>%\n  # FOr model with GY_bu ~ 0 + TREAT\n   mutate(# Rates vs. Check\n          `A - N60_N0` = N60 - N0,\n          `B - N120_N0` = N120 - N0,\n          `C - N180_N0` = N240 - N0,\n          `D - N240_N0` = N240 - N0,\n          .before = 1) %>% \n  dplyr::select(1:4) %>%\n  pivot_longer(cols = everything(),\n               values_to = \"value\", names_to = \"contrast\")\n\n# Contrast summary\ncontrast_summary <- \n  contrast_tibble %>% group_by(contrast) %>% \n             summarise(median = median(value),\n                       q025 = quantile(value, prob = 0.025),\n                       q975 = quantile(value, prob = 0.975) )\n\n\n\n\n\n# Plot\ncontrast_plot <- contrast_tibble %>%\n  ggplot(aes(x = value, color = contrast))+\n  geom_histogram(aes(fill = contrast), color = \"grey25\")+\n  # Add median\n  geom_vline(data = contrast_summary, aes(xintercept = median),\n             linetype = 2, size = 0.75)+\n  # Add limits of credible intervals\n  geom_vline(data = contrast_summary, aes(xintercept = q025),\n             linetype = 3, size = 0.25)+\n  geom_vline(data = contrast_summary, aes(xintercept = q975), \n             linetype = 3, size = 0.25)+\n  # Add line at 0 bu/ac.\n  geom_vline(data = contrast_summary, aes(xintercept = 0), \n             linetype = 1, size = 0.5)+\n  facet_wrap(~contrast, nrow = 2)+\n  labs(title = \"Posteriors of comparisons\",\n       x = \"Difference (bu/ac.)\", y = \"Density\")+\n  theme_tidybayes()+\n  theme(legend.position = \"none\")\n\ncontrast_plot"
  },
  {
    "objectID": "e5_shinyapps.html",
    "href": "e5_shinyapps.html",
    "title": "Week #5",
    "section": "",
    "text": "This article brings my personal experience and resources about this incredible tool called Shiny Apps.\nLet‚Äôs explore it!"
  },
  {
    "objectID": "e5_shinyapps.html#what-is-shiny",
    "href": "e5_shinyapps.html#what-is-shiny",
    "title": "Week #5",
    "section": "WHAT IS SHINY?",
    "text": "WHAT IS SHINY?\n\n\n\n\n\n\nNote\n\n\n\nIt simply consists of a packages family that facilitates the creation of interactive web tools üíª using R! (and recently Python too)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhy is Shiny relevant to us?\nIn simple words, it can make us better data scientists by:\n\nAdding value to our research, increasing the accessibility and visibility. A published paper is great but no the end of the story.\nAllowing stakeholders to explore our results in a flexible platform at their own pace.\nOffering interactive recommendation guidelines.\n\nSee cornyield0N, DONMaiz, SoybeanVRS\n\nFacilitating the use of software development by peers (e.g.¬†demo of R packages)\n\nSee soiltestcorr, metrica"
  },
  {
    "objectID": "e5_shinyapps.html#main-features",
    "href": "e5_shinyapps.html#main-features",
    "title": "Week #5",
    "section": "01. Main features",
    "text": "01. Main features\nShiny has many features that make it unique. My top list is the following:\n\nThe main secret of shiny is the reactive programming. This is what if makes it easy to transform R code into an interactive platform that offers ‚Äúoutputs‚Äù that react to users‚Äô ‚Äúinputs‚Äù.\nA second great feature is a default design of the user interface (‚ö†Ô∏èspoiler alert!) based on Bootstrap, which is the reason why we don‚Äôt directly need to apply HTML, CSS or Java language by ourselves, this tool does it for us.\n\nThe next special feature of shiny is a pre-built collection of widgets that make the app attractive and intuitive (e.g.¬†sliders, check boxes, value boxes, gauges, etc.)\nThen, probably the most important for us (no web developers), is its integration with RMarkdown (& Quarto) through Flexdashboard.\nOf course, as you get more proficient, you will find there are many more features to squeeze‚Ä¶for more information visit https://rstudio.github.io/shiny/"
  },
  {
    "objectID": "e5_shinyapps.html#development-options",
    "href": "e5_shinyapps.html#development-options",
    "title": "Week #5",
    "section": "02. Development options",
    "text": "02. Development options\n\n\n\n\n\n\nImportant\n\n\n\nToday we are going to (briefly) cover the three main development options:\n(i) Base Shiny\n(ii) Flexdashboard\n(iii) UI Editor\n\n\n\n(i) Base Shiny\n\nBase shiny is undoubtedly the most powerful option. However, I personally feel this as the most challenging one in terms of syntax, which is definitely its bottleneck for adoption.\nStructure.\nBase shiny apps are segregated into two main components:\n\nUser Interface (UI)\n\nit controls what and how is being displayed on the application page.\nit receives the users‚Äô input to start with the reactive programming.\nthus, it includes the text, widgets, plots, etc‚Ä¶\n\nServer\n\nit controls the data that is used to produce the outcomes displayed on the UI.\nit normally contains the code to load the libraries, wrangle the data, and define plot functions.\nit borrows input values defined by the UI\n\n\nBoth UI and server could be either included into a single file (App.R) or they can be separated into two files to simplify future changes.\nLet‚Äôs see this with one example\n\n\n\n\n\n\nTip\n\n\n\n\nCreate a new project (and directory) along with a script file named ‚Äúapp.R\" file containing a basic app.\n\nGo to File > New Project > New Directory > Shiny Web Application.\nFile > New File > R Script, or use the shortcut ‚ÄúCtrl + Shift + N‚Äù\nThen add the following lines\n\n\n\n\n\n# library(shiny)\n# ui <- fluidPage(\n#   \"Hello, world! We are Statasaurus!\"\n# )\n# server <- function(input, output) {\n# }\n# shinyApp(ui, server)\n\n\n\nExample with base Shiny\nSHINYAPP: https://ciampittilab.shinyapps.io/soiltestcorr/\nCODE: https://github.com/adriancorrendo/Shiny_soiltestcorr\n\n\n(ii) Flexdashboard\n\nFlexdashboard is a package developed by RStudio that allows to publish interactive dashboards using R Markdown (& Quarto) syntax.\nThe main features that I personally highlight are:\n\nLayout: the easiness to specify the app skeleton using row/column based layouts.\nReport format: the Rmarkdown format allows the keep a similar format used for our data processing and analysis, so we don‚Äôt need to drastically change our usual syntax.\nWebpage: we can develop not only the shiny apps but also webpages, blogs with the shiny apps embedded on them.\nFormatting: As well as base shiny, flexdashboard can optionally apply Bootstrap and bslib to customize colors, fonts, and more.\nDisplay code in place: for teaching, the RMarkdown syntax using chunks allows us to eventually display the code within the same location on the UI.\nUI-Server: There is no need of segregating our code into UI and Server anymore.\n\n\n\n\n\n\n\nWarning\n\n\n\nAdvanced users consider that flexdashboard is limited in comparison to what one can do with base shiny (Shiny & Shinydashboard packages).\n\n\n\n# install.packages(\"flexdashboard\")\n\n\n\nExample with Flexdashboard\nLet‚Äôs see now the metrica app code using Flexdashboard\nSHINYAPP: https://ciampittilab.shinyapps.io/metrica/\nCODE: https://github.com/adriancorrendo/flexdashboard_metrica\n\n\n(ii) UI Editor\nThe UI Editor is a tool currently under development intended to remove barriers on the use of base shiny by non-webpage developers.\n\n# install.packages(\"remotes\")\n\n# Install using the remotes package\n# remotes::install_github(\"rstudio/shinyuieditor\")\n\nIt basically speeds up to process of creating the layout of the UI using an interactive interface. It would be a kind of Shiny App for creating Shiny Apps.\n\nCode template: the UI Editor will create the necessary code in Base Shiny syntax for us (App.R). Later we would just need to fine tune formatting, fonts, colors, etc. This feature is a claear advantage for cases when we would need advanced features into the layout where Flexdashboard could be limited.\n\nKeep tabs on this cause more resources on the UI Editor are coming soon"
  },
  {
    "objectID": "e5_shinyapps.html#deploying-the-shiny-app",
    "href": "e5_shinyapps.html#deploying-the-shiny-app",
    "title": "Week #5",
    "section": "03. Deploying the Shiny App",
    "text": "03. Deploying the Shiny App\nBelieve it or not, publishing our Shiny app is quite easy! (with limitations)\nWe just need to:\n\nCreate an account on shinyapps.io (for free!).\nInstall the rsconnect package in our session.\n\n\n# install.packages(\"rsconnect\")\n\n\nAccount authorization: go to your shinyapps account and copy the TOKEN (showing the secret) provided in your dashboard and run it in your RStudio console at Tools > Global Options > Publishing > Connect ‚Ä¶ > ShinyApps.io.\n\nClick on the ‚ÄòPublish‚Äô button at the top right corner.\nDone!"
  },
  {
    "objectID": "e5_shinyapps.html#resources",
    "href": "e5_shinyapps.html#resources",
    "title": "Week #5",
    "section": "04. Resources",
    "text": "04. Resources\n\nMastering Shiny Book: https://mastering-shiny.org/\n\n\n\nOfficial User Guide: https://docs.rstudio.com/shinyapps.io/\nShiny Tutorial: https://shiny.rstudio.com/tutorial/\nShiny Gallery: https://shiny.rstudio.com/gallery/#demos\nFlexdashboard Shiny Gallery: https://pkgs.rstudio.com/flexdashboard/articles/examples.html"
  },
  {
    "objectID": "e7_metaanalysis.html",
    "href": "e7_metaanalysis.html",
    "title": "Week #8 | Systematic reviews procedures in R",
    "section": "",
    "text": "Lining out the search query one step at a time for REPLICABILITY\nSCOPUS advanced search\nSOURCE-ID (78796 OR 59988 OR 38753 OR 15639) AND\nTITLE( (wheat OR nitrogen) AND (protein OR yield) ) AND\nPUBYEAR > 1980\n\n\nSelect all articles\nClick on ‚ÄúExport CSV‚Äù\nFrom the bulleted list select all those features you will need for further exploration of articles (Title, authors, publication year, abstract, etc)\nDownload files with the extension .ris, which can be handled by revtools package.\n\n\nShow codeknitr::opts_chunk$set(echo = TRUE,  message = FALSE,  warning = FALSE,  tidy = TRUE)\n# Required packages\n#install.packages(\"easypackages\")\nlibrary(easypackages)\npackages(\"tidyverse\")\npackages(\"revtools\")\npackages(\"readxl\")\npackages(\"janitor\")\npackages(\"bayestestR\")\npackages(\"mi\")\npackages(\"metafor\")\npackages(\"multidplyr\")\nsource(\"assets/class07/functions_sys_reviews.R\")\n`%nin%` <- Negate(`%in%`)\n\n\n\nQuery -\nSOURCE-ID (78796 OR 59988 OR 38753 OR 15639) AND\nTITLE( (wheat OR nitrogen) AND (protein OR yield) ) AND\nPUBYEAR > 1980\n\nShow codepath = \"assets/class07/articles/articles already searched/\"\n\nfiles <- list.files(path = path, pattern = \"*.ris\")\n\nfirst_search = load_bibliography(path = path, files = files)\n\n\n\nQuery -\nSOURCE-ID (78796 OR 59988 OR 38753 OR 15639) AND\nTITLE-ABS-KEY( (wheat OR nitrogen) AND (protein OR yield) ) AND\nPUBYEAR > 1980\n\nShow codepath2 = \"assets/class07/articles/new search/\"\n\nfiles2 <- list.files(path = path2, pattern = \"*.ris\")\n\nsecond_search = load_bibliography(path = path2, files = files2)\n\n#View(second_search)\n\n\n\n\nShow codedf_final = anti_join(second_search, first_search)\n\nwrite.csv(x = df_final, file = \"assets/class07/articles/articles_search_final.csv\")\n\n\n\n\nShow code#screen_abstracts(max_file_size = 10)"
  },
  {
    "objectID": "e7_metaanalysis.html#meta-analytic-data",
    "href": "e7_metaanalysis.html#meta-analytic-data",
    "title": "Week #8 | Systematic reviews procedures in R",
    "section": "Meta-analytic data",
    "text": "Meta-analytic data\nAs an example we will use data from split N application in with crops. This meta analysis is comparing whether applying N on a single dose or splitting(2 splits, 3 splits or just split, regardless or number of splits) has any effect on wheat yields.\nIt does also compared how different factors (called moderators if categorical) affect the size of the observed effects.\nThe article can be find here Hu et al 2021\nLoad data and wrangling\n\nShow codedata = read_excel(\"assets/class07/example_data.xlsx\", skip = 2) %>% \n  janitor::clean_names()\n\n\n1. Imputation of missing data\n1.1) Run multiple.imputation()\n\nShow coden.imp = 10\ndf.for.imp = data %>% \n  select(contains(c(\"yield\", \"sd_\")))\n\ndata.imp = data %>% \n  cbind(# Imputation of SD of grain yield when applying N all at once\n        multiple.imputation(n.imp = n.imp, df.variables = df.for.imp, impute.var = \"sd_1\", var.name = \"sd1_imp\"),\n        # Imputation of SD of grain yield when splitting N twice\n        multiple.imputation(n.imp = n.imp, df.variables = df.for.imp, impute.var = \"sd_2\", var.name = \"sd2_imp\")\n  )\n\nNOTE: In the following pairs of variables, the missingness pattern of the second is a subset of the first.\n Please verify whether they are in fact logically distinct variables.\n     [,1]   [,2]     \n[1,] \"sd_3\" \"log_VAR\"\n[2,] \"sd_4\" \"log_VAR\"\nNOTE: In the following pairs of variables, the missingness pattern of the second is a subset of the first.\n Please verify whether they are in fact logically distinct variables.\n     [,1]   [,2]     \n[1,] \"sd_3\" \"log_VAR\"\n[2,] \"sd_4\" \"log_VAR\"\n\n\n2. Calculate effect sizes & pooled sample variance\n\nShow codedata.imp_es = \n  data.imp %>%\n  drop_na(yield_kg_ha_1,reps_1, reps_2, sd1_imp, sd2_imp, yield_kg_ha_2 ) %>% \n  transmute(\n            PAPER_ID = no, \n            TEXTURE = soil_texture, \n            AI = aridity_index, \n            WHEAT_TYPE = whea_type, \n            TILLAGE = tillage,\n            # Response Ratio\n            RR = log(yield_kg_ha_2/yield_kg_ha_1),\n            # Calculate pooled sampling variance\n            VAR = pooled.var(sd.treated = sd2_imp, sd.control  = sd1_imp, \n                          n.control = reps_1, n.treated = reps_2,\n                          m.treated = yield_kg_ha_1, m.control = yield_kg_ha_2),\n            # Weights\n            W = 1/VAR\n            )\n\n\n3. Run pooled model - intercept only\nYou can find more info about I2 statistic here: Borenstein 2015, Higgins and Thompson 2002\n\nShow code# Run pooled model\nmod = rma(yi = RR,\n          vi = VAR,\n          weights = W,\n          #control = list(optimizer=\"optimParallel\", ncpus=3),\n          data = data.imp_es)\n\nsummary(mod)\n\n\nRandom-Effects Model (k = 1311; tau^2 estimator: REML)\n\n    logLik    deviance         AIC         BIC        AICc   \n 1054.4785  -2108.9571  -2104.9571  -2094.6015  -2104.9479   \n\ntau^2 (estimated amount of total heterogeneity): 0.0083 (SE = 0.0004)\ntau (square root of estimated tau^2 value):      0.0913\nI^2 (total heterogeneity / total variability):   92.40%\nH^2 (total variability / sampling variability):  13.16\n\nTest for Heterogeneity:\nQ(df = 1310) = 14454.8167, p-val < .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.0249  0.0046  5.3755  <.0001  0.0158  0.0339  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nShow code# Back transformation\ntrans(coef(mod))\n\n intrcpt \n2.517654 \n\nShow code# I squared statistic\nmod$I2\n\n[1] 92.39838\n\n\n4. Influential Studies Diagnosis\nWhen certain studies excert a strong influence in the model output they are consider influential. An influential case can be diagnosed when the cook‚Äôs D value for a given study is x 3 times greater than the average Cook‚Äôs D of the whole data. Use this citation for the this procedure: Cook 1977, Stephanie 2016\n\nShow code# cooks.distance.rma.uni(model = mod, progbar = T) %>% \n#   saveRDS(\"output/cooksD_diagnosis.RData\")\n# \n# plot(readRDS(\"output/cooksD_diagnosis.RData\"))\n\ninfluential_cases = c(2, 3, 4)\n\ndata.imp_es_ic = \n  data.imp_es %>% \n  mutate(W = case_when(PAPER_ID %in% influential_cases ~ 0, T~W))\n\n\n5. Run non-bootstrapped model\nWeight of influential studies is set to zero\n\nShow codemod2 = rma(yi = RR,\n          vi = VAR,\n          weights = W,\n          mods = ~ 0 + TEXTURE,\n          #control = list(optimizer=\"optimParallel\", ncpus=3),\n          data = data.imp_es_ic)\n\n\n6. Run bootstrapped models\nCitation: Adams et al 1997\n6.1) Pooled effects, intercept only model\n\nShow code#bootstrap_rma(data = data.imp_es_ic, response_variable = \"RR\",moderator = NA, boot_num = 16, cores = 16)\n\n\n6.2) Test potential moderators\n\nShow code#bootstrap_rma(data = data.imp_es_ic, response_variable = \"RR\",moderator = \"TEXTURE\", boot_num = 16, cores = 16)\n\n\n6.3) Summarize bootstraps\n\nShow codedf.plot = summarise_bootstraps(readRDS(\"assets/class07/output/RR_TEXTURE_mod.RData\"))\n\n\n6.4) Plot\n\nShow codedf.plot %>% \n  ggplot()+\n  geom_linerange(aes(ymin = trans(ESTIM_q975), ymax = trans(ESTIM_q025), x = MOD), size = 1)+\n  geom_point(aes(x = MOD, y = trans(ESTIM_q500), fill = MOD), shape = 21, size = 6, stroke = 1.2)+\n  coord_flip()+ \n  labs(x = \"Soil Texture\", y = \"Effect Size (%)\")+\n  guides(fill = \"none\")+\n  theme_bw()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statasaurus‚Äô Homepage",
    "section": "",
    "text": "History\nStatasaurus (a.k.a. Stats-Hub) is a series of encounters designed to share, discuss, learn, and use applied stats and computer programming for agricultural research. It was born in late Fall 2018 from the idea of several lab mates (Adrian Correndo, Santiago Tamagno, Javier Fernandez, and Luiz Moro Rosso) with the support of their advisor (Dr.¬†Ciampitti) at the KSU Crops team (now CiampittiLab).\nStatasaurus was conceived with the main goal sharing knowledge about data analysis using R software. By that time, we were mostly using excel for data wrangling, GraphPad Prism for plots, barely used R script files, no Rmarkdown for reports, and no version control. We didn‚Äôt even know how to create an *.Rproj file. Rapidly, the entire team adopted the idea and the rest is known history. Over the years, many graduate students, visiting scholars, and postdocs have been sharing their coding and stats knowledge (and doubts) allowing the team to exponentially grow in terms of data analytics skills. Nowadays, we are proudly using these meetings exclusively for advanced programming topics. Welcome to the team!\nGeneral Rules\n\n\n\n\n\n\nImportant\n\n\n\nThe #1 and most important rule is: exchange! When you ask for a topic, you also try to offer another topic in exchange.\nThe willing to contribute is of central importance cause we all have duties to take care about. Therefore, self-volunteers are crucial and deeply appreciated.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe meetings are not lectures. Nobody‚Äôs bringing the truth! We are all learning, so we foster the discussion among members.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is expected that members have:\ni. the latest version of RStudio installed.\nii. an active GitHub account, and‚Ä¶\niii. training on the basics of data analysis using R."
  },
  {
    "objectID": "members.html",
    "href": "members.html",
    "title": "Members",
    "section": "",
    "text": "Member\nGitHub user\n\n\n\n\nAdrian Correndo\nhttps://github.com/adriancorrendo\n\n\nIgnacio Massigoge\nhttps://github.com/imassigoge\n\n\nLuciana Nieto\nhttps://github.com/luciananieto\n\n\nValentina Pereyra\nhttps://github.com/valentinapereyra\n\n\nMario Secchi\nhttps://github.com/mariosecchi\n\n\nJosefina Lacasa\nhttps://github.com/jlacasa\n\n\nLuiz Felipe\nhttps://github.com/luizfelipeaa\n\n\nJuan Rybecky\nhttps://github.com/juanrybecky\n\n\nEmma van Versendaal\nhttps://github.com/emmavanver\n\n\nFrancisco Palmero\nhttps://github.com/fpalmeroo\n\n\nDanilo Tedesco\nhttps://github.com/tedescodanilo\n\n\nNilson Vieira\nhttps://github.com/nvieirajr\n\n\nLucia Marziote\nhttps://github.com/Luciamar\n\n\nCarlos Hernandez\nhttps://github.com/Carlitosh\n\n\nAna Carcedo\nhttps://github.com/anajpcarcedo\n\n\nNicolas Giordano\nhttps://github.com/giordanon\n\n\nLara Zini\nhttps://github.com/larazini\n\n\nGustavo Santiago\nhttps://github.com/GustavoSantiago1103\n\n\nIgnacio Rodriguez\nhttps://github.com/nachorodr\n\n\nPia Rodriguez\nhttps://github.com/piarod1r\n\n\nThomas Varela\nhttps://github.com/thomasvarela\n\n\nLeonardo Bosche\n-\n\n\nAgustina Go√±i\n-\n\n\nYesica Chazarreta\n-"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Fall 2022 - Weekly meeting schedule\n\n\nüóìÔ∏èDate |\nü•ïTopic |\nüôãüèªMembers\n| LINKS \n\n\n\n\n29-08-2022\nIntro, GitHub, Quarto\nAdrian\nhttps://github.com/adriancorrendo/Stats_Hub_2022\n\n\n05-09-2022\nLabor Day\n-\n\n\n\n12-09-2022\nHierarchical Bayesian Models I - Theory & Discussion\nAdrian and Josefina\nhttps://adriancorrendo.github.io/statasaurusweb/e2_bayes_1.html\n\n\n19-09-2022\nHierarchical Bayesian Models II - Applied Examples with rjags, rstan , brms \nAdrian and Josefina\nhttps://adriancorrendo.github.io/statasaurusweb/e3_bayes_2.html\n\n\n26-09-2022\nHierarchical Bayesian Models III\nAdrian and Josefina\nhttps://adriancorrendo.github.io/statasaurusweb/e4_bayes_3.html\n\n\n03-10-2022\nShinyapps \nAdrian\nhttps://adriancorrendo.github.io/statasaurusweb/e5_shinyapps.html\n\n\n10-10-2022\nRun APSIM in R, apsimx,\nlapply() style\nAna\nhttps://github.com/adriancorrendo/Stats_Hub_2022/tree/main/apsim\n\n\n17-10-2022\nRun APSIM in R II - Crop rotation,\npmap() style\nAna, Ignacio, Emma\nhttps://github.com/adriancorrendo/apsiminR/\n\n\n24-10-2022\nSystematic Reviews,\nMeta-analysis protocols\nNicolas\nhttps://adriancorrendo.github.io/statasaurusweb/e7_metaanalysis.html\n\n\n31-10-2022\nAdvanced ggplot ,\nData digitization packages (juicr)\nAdrian, Emma, Juan\n\n\n\n07-11-2022\nASA Meetings‚Äô Week\n-\n\n\n\n14-11-2022\nMachine Learning, tidymodels, H2O , Prediction Error Metrics, metrica \nAdrian, Luciana\n\n\n\n21-11-2022\nThanksgiving break\n-\n\n\n\n28-11-2022\nOpen discussion session\n-\n\n\n\n05-12-2022\nCreating a package, devtools , roxygen2, GitHub Continuous Integration, CRAN\nAdrian\n\n\n\n12-12-2022\nConclusions & next steps\n-"
  }
]